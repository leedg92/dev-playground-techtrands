from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

from datetime import datetime, timedelta

import requests
import json
import concurrent.futures
import html
import re
import time
import os

# ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from bs4 import BeautifulSoup

# DB ì—°ê²°
from psycopg2 import connect
from psycopg2.extras import execute_values
from psycopg2.extensions import adapt
from psycopg2.extensions import register_adapter

# spaCy ëª¨ë¸ ë° VADER ê°ì„±ë¶„ì„ê¸° ì´ˆê¸°í™” (ì „ì—­ë³€ìˆ˜ë¡œ í•œë²ˆë§Œ ë¡œë“œ)
nlp = spacy.load("en_core_web_sm")  # pip install en_core_web_sm í•„ìš”
analyzer = SentimentIntensityAnalyzer()

# ê¸°ìˆ  í‚¤ì›Œë“œ ì‚¬ì „ (HackerNewsì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ëŠ” ê¸°ìˆ  ìš©ì–´ë“¤)
ALL_TECH_KEYWORDS = set()  # ëª¨ë“  ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ setìœ¼ë¡œ ê´€ë¦¬


"""

1. ëª¨ë“  ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ setìœ¼ë¡œ í†µí•©
2. ìµœê·¼ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
3. ìµœê·¼ ê²Œì‹œê¸€ ëª©ë¡ ì¤‘ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²Œì‹œê¸€ ëª©ë¡ ì¶”ì¶œ
4. ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²Œì‹œê¸€ ëª©ë¡ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ (XComìœ¼ë¡œ ì „ë‹¬)
5. ê°ì„±ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ìˆ  í‚¤ì›Œë“œ ìš°ì„ )
6. ê°ì„±ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ ì¶œë ¥
7. ê°ì„±ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
"""

    


# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ (HTML ë””ì½”ë”© ë° ì •ë¦¬)
def clean_text(text):
    """
    HTML ì¸ì½”ë”© ë¬¸ì œ í•´ê²° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
    """
    if not text or text == 'None':
        return ""
    
    # HTML ì—”í‹°í‹° ë””ì½”ë”© (&quot; â†’ ", &#x27; â†’ ' ë“±)
    text = html.unescape(text)
    
    # HTML íƒœê·¸ ì œê±° (BeautifulSoup ì‚¬ìš©)
    soup = BeautifulSoup(text, 'html.parser')
    text = soup.get_text()
    
    # ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë‹¨, í•˜ì´í”ˆê³¼ ì–¸ë”ìŠ¤ì½”ì–´ëŠ” ìœ ì§€)
    text = re.sub(r'[^\w\s\-_]', ' ', text)
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


# ëª¨ë“  ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ì˜ setìœ¼ë¡œ í†µí•©
def set_all_tech_keywords():
    try:
        conn = connect(
            host=os.environ['HOST'],
            database=os.environ['DATABASE'],
            user=os.environ['USER'], 
            password=os.environ['PASSWORD'],
            port=os.environ['PORT']
        )
        print("âœ… DB ì—°ê²° ì„±ê³µ!")
        
        cur = conn.cursor()
        print("ğŸ“ ì»¤ì„œ ìƒì„± ì™„ë£Œ")
        
        select_query = """
        SELECT keyword FROM tech_trends.tech_dictionary
        """
        cur.execute(select_query)
        all_tech_keywords = cur.fetchall()
        for keyword in all_tech_keywords:
            ALL_TECH_KEYWORDS.add(keyword[0].lower())
        print(f"ğŸ” ëª¨ë“  ê¸°ìˆ  í‚¤ì›Œë“œ ì„¸íŒ… ì™„ë£Œ: {len(ALL_TECH_KEYWORDS)}ê°œ")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise e
    finally:
        if cur:
            cur.close()
            print("\nğŸ”„ DB ì»¤ì„œ ì¢…ë£Œ") 
            print(ALL_TECH_KEYWORDS) 


# ìµœê·¼ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
def get_new_story_ids():
    # ì €ì¥ëœ ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (DAG Run ê°„ ë°ì´í„° ê³µìœ )
    try:
        processed_story_ids = json.loads(Variable.get("processed_story_ids", default_var="[]"))
        print("ì´ì „ì— ì²˜ë¦¬ëœ ê²Œì‹œê¸€:", len(processed_story_ids), "ê°œ")
    except:
        processed_story_ids = []
    
    # ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ ëª©ë¡ API í˜¸ì¶œí•˜ì—¬ ID ìˆ˜ì§‘
    story_ids = set()
    
    # ê° API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
    api_endpoints = [
        'topstories',    # ìµœê³  ì¸ê¸° ìŠ¤í† ë¦¬
        'newstories',    # ìµœì‹  ìŠ¤í† ë¦¬
        'beststories',   # ë² ìŠ¤íŠ¸ ìŠ¤í† ë¦¬
        'askstories',    # Ask HN ìŠ¤í† ë¦¬
        'showstories'    # Show HN ìŠ¤í† ë¦¬
    ]
    
    for endpoint in api_endpoints:
        response = requests.get(f'https://hacker-news.firebaseio.com/v0/{endpoint}.json')
        story_ids.update(response.json())  # setì— ì¤‘ë³µì—†ì´ ì¶”ê°€
    
    all_story_ids = list(story_ids)  # setì„ listë¡œ ë³€í™˜
    
    # ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²Œì‹œê¸€ë§Œ ì°¾ê¸° (ì°¨ì§‘í•© ì—°ì‚°)
    new_story_ids = [story_id for story_id in all_story_ids if story_id not in processed_story_ids]
    print("ìƒˆë¡œ ì²˜ë¦¬í•  ê²Œì‹œê¸€:", len(new_story_ids), "ê°œ")
    
    return new_story_ids

# ê²Œì‹œê¸€ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ë° XComì— ì €ì¥
def get_story_detail(**context):
    # Xcomì—ì„œ ì²˜ë¦¬í•  ê²Œì‹œê¸€ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    new_story_ids = context['ti'].xcom_pull(task_ids='get_new_story_ids')
    stories_data = []  # XComìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    
    # ìµœëŒ€ 10ê°œ ê²Œì‹œê¸€ë§Œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš© ì œí•œ)
    for i in new_story_ids:
        # ê°œë³„ ê²Œì‹œê¸€ ìƒì„¸ì •ë³´ API í˜¸ì¶œ
        response = requests.get(f'https://hacker-news.firebaseio.com/v0/item/{i}.json')
        story_data = response.json()
        
        # ì‚­ì œëœ ê²Œì‹œê¸€ ì²´í¬ (dead í”Œë˜ê·¸)
        isDead = story_data.get('dead', False)
        if not isDead:
            # í•„ìš”í•œ ë°ì´í„°ë§Œ ì¶”ì¶œ
            title = story_data.get('title', 'None')
            text = story_data.get('text', 'None')  # ê²Œì‹œê¸€ ë³¸ë¬¸ (ìˆëŠ” ê²½ìš°ë§Œ)
            url = story_data.get('url', '')
            score = story_data.get('score', 0)
            type = story_data.get('type')
            kids = story_data.get('kids', [])
            for kid in kids:
                kid_data = requests.get(f'https://hacker-news.firebaseio.com/v0/item/{kid}.json')
                kid_data = kid_data.json()
                if kid_data.get('type') == 'comment':
                    comment_id = kid_data.get('id')
                    comment_parent_title = title
                    comment_text = kid_data.get('text', 'None')
                    comment_url = None
                    comment_score = 0
                    comment_author = kid_data.get('by', 'Unknown')
                    comment_type = kid_data.get('type')
                    comment_parent_id = kid_data.get('parent')
                    comment_detail = {
                        'id': comment_id,
                        'title': comment_parent_title,
                        'text': comment_text,
                        'url': comment_url,
                        'score': comment_score,
                        'author': comment_author,
                        'type': comment_type,
                        'parent_id': comment_parent_id
                    }
                    stories_data.append(comment_detail)
                    print(f"{i}ë²ˆ ê²Œì‹œê¸€ ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ: ID {comment_id} - {comment_parent_title[:50]}...")

            
            # ìƒì„¸ì •ë³´ êµ¬ì¡°ì²´ ìƒì„±
            story_detail = {
                'id': i,
                'title': title,
                'text': text,
                'url': url,
                'score': score,
                'author': story_data.get('by', 'Unknown'),
                'type': type,
                'parent_id': None
            }
            
            stories_data.append(story_detail)
            print(f"ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: ID {i} - {title[:50]}...")
    
    print(f"ì´ {len(stories_data)}ê°œ ê²Œì‹œê¸€ ìƒì„¸ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # XComìœ¼ë¡œ ë‹¤ìŒ Taskì— ë°ì´í„° ì „ë‹¬ (return ë°©ì‹)
    return stories_data

# ê¸°ìˆ  í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš© í•¨ìˆ˜
def apply_tech_weights(keywords_with_scores):
    """
    ê¸°ìˆ  í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ì—¬ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì¶œë˜ë„ë¡ í•¨
    """
    weighted_keywords = []
    
    for keyword, score in keywords_with_scores:
        # ê¸°ìˆ  í‚¤ì›Œë“œì¸ì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ê´€)
        if keyword.lower() in ALL_TECH_KEYWORDS:
            # ê¸°ìˆ  í‚¤ì›Œë“œëŠ” 3ë°° ê°€ì¤‘ì¹˜ ì ìš©
            weighted_keywords.append((keyword, score * 3.0))
            print(f"ğŸ”§ ê¸°ìˆ  í‚¤ì›Œë“œ ë°œê²¬: {keyword} (ê°€ì¤‘ì¹˜ ì ìš©)")
        else:
            weighted_keywords.append((keyword, score))
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì¬ì •ë ¬
    return sorted(weighted_keywords, key=lambda x: x[1], reverse=True)

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (spaCy + TF-IDF í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ + ê¸°ìˆ  í‚¤ì›Œë“œ ìš°ì„ )
def extract_keywords(text):
    """
    spaCyì™€ TF-IDFë¥¼ ì¡°í•©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ + ê¸°ìˆ  í‚¤ì›Œë“œ ìš°ì„  ì²˜ë¦¬
    """
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (HTML ë””ì½”ë”© ë“±)
    clean_text_data = clean_text(text)
    
    # spaCyë¡œ í…ìŠ¤íŠ¸ ë¶„ì„ (í† í°í™”, í’ˆì‚¬ íƒœê¹…, ë¶ˆìš©ì–´ ì œê±°)
    doc = nlp(clean_text_data)
    keywords_spacy = []
    
    # ëª…ì‚¬, ê³ ìœ ëª…ì‚¬, í˜•ìš©ì‚¬ë§Œ í‚¤ì›Œë“œ í›„ë³´ë¡œ ì¶”ì¶œ
    for token in doc:
        if (token.pos_ in ['NOUN', 'PROPN', 'ADJ'] and  # í’ˆì‚¬ í•„í„°ë§
            not token.is_stop and                        # ë¶ˆìš©ì–´ ì œê±°
            not token.is_punct and                       # êµ¬ë‘ì  ì œê±°
            len(token.text) > 2 and                      # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œê±°
            not token.text.isdigit()):                   # ìˆ«ìë§Œ ìˆëŠ” í† í° ì œê±°
            keywords_spacy.append(token.lemma_.lower())   # ì›í˜•ìœ¼ë¡œ ë³€í™˜ í›„ ì†Œë¬¸ìí™”
    
    # TF-IDFë¡œ í‚¤ì›Œë“œ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
    if len(keywords_spacy) > 0:
        try:
            # TF-IDF ë²¡í„°í™” (ìµœëŒ€ 15ê°œ íŠ¹ì„±, ì˜ì–´ ë¶ˆìš©ì–´ ì œê±°)
            vectorizer = TfidfVectorizer(max_features=15, stop_words='english')
            tfidf_matrix = vectorizer.fit_transform([clean_text_data])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]
            
            # ëª¨ë“  í‚¤ì›Œë“œì™€ ì ìˆ˜ ì¶”ì¶œ
            all_keywords = [(feature_names[i], tfidf_scores[i]) for i in range(len(feature_names))]
            
            # ê¸°ìˆ  í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
            weighted_keywords = apply_tech_weights(all_keywords)
            
            # ìƒìœ„ 8ê°œ í‚¤ì›Œë“œ ì„ íƒ
            top_keywords = weighted_keywords[:8]
            
        except Exception as e:
            print(f"TF-IDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # TF-IDF ì‹¤íŒ¨ì‹œ spaCy í‚¤ì›Œë“œ ìƒìœ„ 8ê°œ ì‚¬ìš©
            spacy_keywords = [(kw, 0.5) for kw in list(set(keywords_spacy))[:8]]
            top_keywords = apply_tech_weights(spacy_keywords)
    else:
        top_keywords = []
    
    return top_keywords

# ê°ì„±ë¶„ì„ í•¨ìˆ˜ (VADER í™œìš©)
def analyze_sentiment(text):
    """
    VADERë¥¼ ì‚¬ìš©í•œ ê°ì„±ë¶„ì„
    """
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    clean_text_data = clean_text(text)
    
    # VADERë¡œ ê°ì„± ì ìˆ˜ ê³„ì‚°
    sentiment_scores = analyzer.polarity_scores(clean_text_data)
    
    # compound ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°ì„± ë¼ë²¨ ë¶„ë¥˜
    compound = sentiment_scores['compound']
    if compound >= 0.05:      # ê¸ì •ì 
        sentiment_label = 'positive'
    elif compound <= -0.05:   # ë¶€ì •ì 
        sentiment_label = 'negative'
    else:                     # ì¤‘ë¦½ì 
        sentiment_label = 'neutral'
    
    return {
        'label': sentiment_label,
        'compound': compound,
        'positive': sentiment_scores['pos'],
        'neutral': sentiment_scores['neu'],
        'negative': sentiment_scores['neg']
    }

# ê°ì„±ë¶„ì„ + í‚¤ì›Œë“œ ì¶”ì¶œ í†µí•© í•¨ìˆ˜
def analyze_content(title, text):
    """
    ê²Œì‹œê¸€ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê°ì„±ë¶„ì„ ìˆ˜í–‰
    """
    # ì œëª©ê³¼ ë³¸ë¬¸ì„ í•©ì³ì„œ ë¶„ì„ (ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì¶”ê°€, ì—†ìœ¼ë©´ ì œëª©ë§Œ)
    combined_text = title
    if text and text != 'None' and text.strip():
        combined_text = f"{title} {text}"
        print(f"ğŸ“„ ë³¸ë¬¸ í¬í•¨ ë¶„ì„: ì œëª© + ë³¸ë¬¸ ({len(text)} ê¸€ì)")
    else:
        print(f"ğŸ“„ ì œëª©ë§Œ ë¶„ì„: {title}")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰ (ê¸°ìˆ  í‚¤ì›Œë“œ ìš°ì„ )
    keywords = extract_keywords(combined_text)
    
    # ê°ì„±ë¶„ì„ ì‹¤í–‰
    sentiment = analyze_sentiment(combined_text)
    
    # ê²°ê³¼ êµ¬ì¡°ì²´ ìƒì„±
    analysis_result = {
        'keywords': [kw[0] for kw in keywords],           # í‚¤ì›Œë“œ ëª©ë¡
        'keyword_scores': dict(keywords),                 # í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„ ì ìˆ˜
        'sentiment': sentiment,                           # ê°ì„±ë¶„ì„ ê²°ê³¼
        'has_text': text and text != 'None' and text.strip()  # ë³¸ë¬¸ ì¡´ì¬ ì—¬ë¶€
    }
    
    return analysis_result

# ê°ì„±ë¶„ì„ + í‚¤ì›Œë“œ ì¶”ì¶œ ë©”ì¸ Task í•¨ìˆ˜
def process_sentiment_and_keywords(**context):
    """
    XComì—ì„œ ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ ë°›ì•„ì™€ì„œ ê°ì„±ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ ìˆ˜í–‰
    ê¸°ìˆ  í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²Œì‹œê¸€ì€ ë¶„ì„ì—ì„œ ë°°ì œ
    """
    # XComì—ì„œ ì´ì „ Taskì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    stories_data = context['ti'].xcom_pull(task_ids='get_story_detail')
    
    # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if not stories_data:
        print("ë¶„ì„í•  ê²Œì‹œê¸€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ¤– ê°ì„±ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘: {len(stories_data)}ê°œ ê²Œì‹œê¸€")
    print("ğŸ” ê¸°ìˆ  í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²Œì‹œê¸€ë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")
    print("=" * 80)
    
    # ê¸°ìˆ  í‚¤ì›Œë“œ í†µê³„
    tech_keyword_count = 0
    total_keyword_count = 0
    analyzed_stories = 0  # ì‹¤ì œ ë¶„ì„ëœ ê²Œì‹œê¸€ ìˆ˜
    skipped_stories = 0   # ê±´ë„ˆë›´ ê²Œì‹œê¸€ ìˆ˜

    tech_trends_data = []
    
    # ê° ê²Œì‹œê¸€ì— ëŒ€í•´ ë¶„ì„ ìˆ˜í–‰
    for idx, story in enumerate(stories_data, 1):
        print(f"\n[{idx}/{len(stories_data)}] ê²Œì‹œê¸€ ì˜ˆë¹„ ë¶„ì„ ì¤‘...")
        print(f"ğŸ†” ID: {story['id']}")
        print(f"ğŸ“° ì œëª©: {story['title']}")
        
        # ë¨¼ì € í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ê¸°ìˆ  í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        combined_text = story['title']
        if story['text'] and story['text'] != 'None' and story['text'].strip():
            combined_text = f"{story['title']} {story['text']}"
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ìˆ  í‚¤ì›Œë“œ ìš°ì„ )
        keywords = extract_keywords(combined_text)
        
        # ê¸°ìˆ  í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        tech_keywords = []
        for kw, _ in keywords:
            if kw.lower() in ALL_TECH_KEYWORDS:
                tech_keywords.append(kw)
        
        # ê¸°ìˆ  í‚¤ì›Œë“œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if not tech_keywords:
            print(f"âŒ ê¸°ìˆ  í‚¤ì›Œë“œ ì—†ìŒ - ë¶„ì„ ê±´ë„ˆë›°ê¸°")
            print(f"   ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join([kw[0] for kw in keywords[:3]])}...")
            skipped_stories += 1
            print("-" * 60)
            continue
        
        # ê¸°ìˆ  í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì „ì²´ ë¶„ì„ ì§„í–‰
        print(f"âœ… ê¸°ìˆ  í‚¤ì›Œë“œ ë°œê²¬ - ì „ì²´ ë¶„ì„ ì§„í–‰")
        print(f"ğŸ‘¤ ì‘ì„±ì: {story['author']}")
        print(f"â¬†ï¸ ì ìˆ˜: {story['score']}")
        
        # ê°ì„±ë¶„ì„ ì‹¤í–‰
        sentiment = analyze_sentiment(combined_text)
        
        # ë¶„ì„ ê²°ê³¼ êµ¬ì„±
        analysis_result = {
            'keywords': [kw[0] for kw in keywords],
            'keyword_scores': dict(keywords),
            'sentiment': sentiment,
            'has_text': story['text'] and story['text'] != 'None' and story['text'].strip()
        }
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        tech_keyword_count += len(tech_keywords)
        total_keyword_count += len(analysis_result['keywords'])
        analyzed_stories += 1
        
        # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ”‘ ì „ì²´ í‚¤ì›Œë“œ: {', '.join(analysis_result['keywords'])}")
        print(f"ğŸ”§ ê¸°ìˆ  í‚¤ì›Œë“œ: {', '.join(tech_keywords)} ({len(tech_keywords)}ê°œ)")
        print(f"ğŸ’­ ê°ì„±: {analysis_result['sentiment']['label']} (ì ìˆ˜: {analysis_result['sentiment']['compound']:.3f})")
        
        tech_trends_data.append({
            'id': story['id'],
            'item_type': story['type'],
            'text_content': story['text'] if story['text'] and story['text'] != 'None' and story['text'].strip() else story['title'],
            'sentiment_score': analysis_result['sentiment']['compound'],
            'sentiment_label': analysis_result['sentiment']['label'],
            'extracted_keywords': next((kw for kw, score in analysis_result['keyword_scores'].items() if kw.lower() in ALL_TECH_KEYWORDS), ''),
            'author': story['author'],
            'created_at': datetime.now(),
            'score': story['score'],
            'parent_id': story['parent_id'],
            'root_story_id': 0
        })

        # í‚¤ì›Œë“œë³„ ì¤‘ìš”ë„ ì ìˆ˜ ì¶œë ¥ (ìƒìœ„ 4ê°œë§Œ)
        if analysis_result['keyword_scores']:
            top_keywords = list(analysis_result['keyword_scores'].items())[:4]
            keyword_str = ', '.join([f"{kw}({score:.2f})" for kw, score in top_keywords])
            print(f"ğŸ“Š ì£¼ìš” í‚¤ì›Œë“œ ì ìˆ˜: {keyword_str}")
        
        # ë³¸ë¬¸ í¬í•¨ ì—¬ë¶€ í‘œì‹œ
        if analysis_result['has_text']:
            print("ğŸ“ ë³¸ë¬¸ ë°ì´í„° í¬í•¨ë¨")
            print(f"ğŸ“„ ë³¸ë¬¸ í¬í•¨ ë¶„ì„: ì œëª© + ë³¸ë¬¸ ({len(story['text'])} ê¸€ì)")
        else:
            print(f"ğŸ“„ ì œëª©ë§Œ ë¶„ì„: {story['title']}")
        
        print("-" * 60)
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    tech_ratio = (tech_keyword_count / total_keyword_count * 100) if total_keyword_count > 0 else 0
    analyzed_ratio = (analyzed_stories / len(stories_data) * 100) if len(stories_data) > 0 else 0
    
    print(f"\nğŸ“ˆ ë¶„ì„ ì™„ë£Œ í†µê³„:")
    print(f"   ì „ì²´ ê²Œì‹œê¸€: {len(stories_data)}ê°œ")
    print(f"   ë¶„ì„ëœ ê²Œì‹œê¸€: {analyzed_stories}ê°œ ({analyzed_ratio:.1f}%)")
    print(f"   ê±´ë„ˆë›´ ê²Œì‹œê¸€: {skipped_stories}ê°œ (ê¸°ìˆ  í‚¤ì›Œë“œ ì—†ìŒ)")
    print(f"   ì´ í‚¤ì›Œë“œ: {total_keyword_count}ê°œ")
    print(f"   ê¸°ìˆ  í‚¤ì›Œë“œ: {tech_keyword_count}ê°œ ({tech_ratio:.1f}%)")
    print(f"   ê¸°ìˆ  í‚¤ì›Œë“œ ë°€ë„: {tech_keyword_count/analyzed_stories:.1f}ê°œ/ê²Œì‹œê¸€" if analyzed_stories > 0 else "   ê¸°ìˆ  í‚¤ì›Œë“œ ë°€ë„: 0ê°œ/ê²Œì‹œê¸€")
    print(f"\nğŸ¯ ê²°ê³¼: ê¸°ìˆ  ê´€ë ¨ ê²Œì‹œê¸€ë§Œ ì„ ë³„ ë¶„ì„í•˜ì—¬ ë” ì •í™•í•œ ê¸°ìˆ  íŠ¸ë Œë“œ íŒŒì•…!")
    
    print(f"   ê¸°ìˆ  ê²Œì‹œê¸€ í•„í„°ë§ ì„±ê³µë¥ : {analyzed_ratio:.1f}%")

    return tech_trends_data


def insert_tech_trends_data(**context):
    cur = None
    conn = None
    print("\nğŸ—ƒï¸ DB ì €ì¥ ì‘ì—… ì‹œì‘...")
    
    # XComì—ì„œ ë¶„ì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    tech_trends_data = context['ti'].xcom_pull(task_ids='process_sentiment_and_keywords')
    if not tech_trends_data:
        all_story_ids = json.loads(Variable.get("processed_story_ids", default_var="[]"))
        new_story_ids = context['ti'].xcom_pull(task_ids='get_new_story_ids')
        all_story_ids.extend(new_story_ids)  # í•„í„°ë§ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ëª¨ë“  ID ì¶”ê°€
        Variable.set("processed_story_ids", json.dumps(list(set(all_story_ids))))  # ì¤‘ë³µ ì œê±°
        print(f"ì²˜ë¦¬ëœ ê²Œì‹œê¸€ ID ëª©ë¡: {all_story_ids}")
        print("ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ“¥ XComì—ì„œ {len(tech_trends_data)}ê°œ ê²Œì‹œê¸€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    
    print("\nğŸ”Œ PostgreSQL DB ì—°ê²° ì‹œë„...")
    try:
        conn = connect(
            host=os.environ['HOST'],
            database=os.environ['DATABASE'],
            user=os.environ['USER'], 
            password=os.environ['PASSWORD'],
            port=os.environ['PORT']
        )
        print("âœ… DB ì—°ê²° ì„±ê³µ!")
        
        cur = conn.cursor()
        print("ğŸ“ ì»¤ì„œ ìƒì„± ì™„ë£Œ")
        
        # SQL ì¿¼ë¦¬ ì¤€ë¹„
        insert_query = """
        INSERT INTO tech_trends.tech_trends_items (
            id, 
            item_type, 
            text_content, 
            sentiment_score, 
            sentiment_label,
            extracted_keywords, 
            author, 
            created_at, 
            score, 
            parent_id, 
            root_story_id
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        print("\nğŸ’¾ ë°ì´í„° ì €ì¥ ì‹œì‘...")
        print("=" * 60)
        
        # ë°ì´í„° ì‚½ì… ì‹¤í–‰
        # ë”•ì…”ë„ˆë¦¬ë¥¼ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í•œ ì¤„ë¡œ)
        data_tuples = [
            (
                item['id'],
                item['item_type'], 
                item['text_content'],
                item['sentiment_score'],
                item['sentiment_label'],
                item['extracted_keywords'],
                item['author'],
                item['created_at'],
                item['score'],
                item['parent_id'],
                item['root_story_id']
            )
            for item in tech_trends_data
        ]

        # ë°ì´í„° ì‚½ì… ì‹¤í–‰
        cur.executemany(insert_query, data_tuples)
        print(f"âœ¨ {len(tech_trends_data)}ê°œ ê²Œì‹œê¸€ ë°ì´í„° ì‚½ì… ì™„ë£Œ!")
        
        # ì»¤ë°‹
        conn.commit()
        print("âœ… íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì™„ë£Œ")
        
        # ì €ì¥ëœ ë°ì´í„° ìš”ì•½ ì¶œë ¥
        for idx, data in enumerate(tech_trends_data, 1):
            print(f"\nğŸ“ ê²Œì‹œê¸€ {idx}/{len(tech_trends_data)}")
            print(f"   ì œëª©: {data['text_content'][:50]}...")
            print(f"   ì‘ì„±ì: {data['author']}")
            print(f"   ê°ì„±: {data['sentiment_label']} ({data['sentiment_score']:.2f})")
            
        # ëª¨ë“  ê²Œì‹œê¸€ IDë¥¼ processed_story_idsì— ì¶”ê°€
        try:
            all_story_ids = json.loads(Variable.get("processed_story_ids", default_var="[]"))
            new_story_ids = context['ti'].xcom_pull(task_ids='get_new_story_ids')
            all_story_ids.extend(new_story_ids)  # í•„í„°ë§ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ëª¨ë“  ID ì¶”ê°€
            Variable.set("processed_story_ids", json.dumps(list(set(all_story_ids))))  # ì¤‘ë³µ ì œê±°
            print("\nâœ… processed_story_ids ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            print(f"\nâŒ processed_story_ids ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise e
        
    finally:
        # ì—°ê²° ì¢…ë£Œ
        if cur:
            cur.close()
            print("\nğŸ”„ DB ì»¤ì„œ ì¢…ë£Œ")
        if conn:
            conn.close() 
            print("ğŸ”Œ DB ì—°ê²° ì¢…ë£Œ")
    
    print("\nâœ… DB ì €ì¥ ì‘ì—… ì™„ë£Œ!")
    print("=" * 60)



default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 7, 15),
    'retries': 0,
    'retry_delay': 0
}

dag = DAG(
    'HACKER_NEWS_TECH_ANALYSIS_DAG',
    default_args=default_args,
    description='Hacker News ê¸°ìˆ  íŠ¸ë Œë“œ ê°ì„±ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ DAG (ê¸°ìˆ  ê²Œì‹œê¸€ í•„í„°ë§)',
    schedule_interval='*/30 * * * *',  # 20ë¶„ì— í•œë²ˆì”© ì‹¤í–‰
    catchup=False
)



# 1ë‹¨ê³„: ëª¨ë“  ê¸°ìˆ  í‚¤ì›Œë“œ ì„¸íŒ…
set_all_tech_keywords_task = PythonOperator(
    task_id='set_all_tech_keywords',
    python_callable=set_all_tech_keywords,
    dag=dag
)

# 2ë‹¨ê³„: ìƒˆë¡œìš´ ê²Œì‹œê¸€ ID ìˆ˜ì§‘
new_story_ids_task = PythonOperator(
    task_id='get_new_story_ids',
    python_callable=get_new_story_ids,
    dag=dag
)

# 3ë‹¨ê³„: ê²Œì‹œê¸€ ìƒì„¸ì •ë³´ ìˆ˜ì§‘
story_detail_task = PythonOperator(
    task_id='get_story_detail',
    python_callable=get_story_detail,
    dag=dag
)

# 4ë‹¨ê³„: ê¸°ìˆ  ê²Œì‹œê¸€ í•„í„°ë§ ë° ë¶„ì„
analysis_task = PythonOperator(
    task_id='process_sentiment_and_keywords',
    python_callable=process_sentiment_and_keywords,
    dag=dag
)

# 5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
insert_tech_trends_data_task = PythonOperator(
    task_id='insert_tech_trends_data',
    python_callable=insert_tech_trends_data,
    dag=dag
)

# Task ì‹¤í–‰ ìˆœì„œ ì •ì˜ (1ë‹¨ê³„ â†’ 2ë‹¨ê³„ â†’ 3ë‹¨ê³„ â†’ 4ë‹¨ê³„ â†’ 5ë‹¨ê³„)
set_all_tech_keywords_task >> new_story_ids_task >> story_detail_task >> analysis_task >> insert_tech_trends_data_task